# Google ADK Evaluation Example

This is an example project that demonstrates how to build, test, and evaluate AI agents using the Google Agent Development Kit (ADK).

The project includes a simple "Weather Agent" that can report weather conditions based on a user-provided city name. The main purpose of this project is to demonstrate the powerful evaluation features of the ADK and to provide a clear reference for project structure.

This project contains two different language versions of the agent:
1.  `weather_agent/` - Traditional Chinese version
2.  `weather_agent_en/` - English version

---

## Project Structure

```
/
├── weather_agent/                  # Traditional Chinese Weather Agent
│   ├── agent.py                    # Agent's core logic (instructions, tools)
│   ├── weather_evaluation_zhtw.evalset.json  # Traditional Chinese evaluation set
│   └── .adk/
│       └── eval_history/           # Storage for historical evaluation results
│
├── weather_agent_en/               # English Weather Agent
│   ├── agent.py                    # Agent's core logic (English version)
│   └── Weather_evaluation.evalset.json # English evaluation set
│
├── .gitignore
├── LICENSE
└── README.md                       # This README file
```

-   **`agent.py`**: Defines the agent's behavior, including its core instruction and available tools, such as the `query_weather` function.
-   **`*.evalset.json`**: The evaluation set file. It contains a series of test cases (eval_cases), each with a user input and an expected ideal response. The ADK uses this file to automatically test the agent's performance.
-   **`.adk/eval_history/`**: A directory automatically generated by the ADK. Each time you run an evaluation, a detailed result report (in JSON format) is stored here for tracking and analysis.

---

## Key Features

-   A simple weather query agent built with `google-adk`.
-   Demonstrates how to define clear instructions and tools for an agent.
-   Provides a structured evaluation set (`.evalset.json`) to enable automated testing and performance assessment.
-   Includes multilingual examples (English and Traditional Chinese) to showcase the potential of ADK in internationalized applications.
-   Offers a standard ADK agent project structure that can serve as a starting point for your future development.

---

## How to Use

### Prerequisites

1.  **Python**: Python 3.11 or newer is recommended.
2.  **Install Google ADK**:
    ```bash
    pip install google-adk
    ```

### Running Evaluation

The core of this project is to demonstrate the evaluation process. You can use the `adk eval` command to automatically test the agent's performance.

-   **Evaluate the Traditional Chinese Agent**:

    Run the following command. The ADK will read all test cases from `weather_evaluation_zhtw.evalset.json` and test the `weather_agent`'s responses one by one.

    ```bash
    adk eval weather_agent/ weather_agent/weather_evaluation_zhtw.evalset.json --config_file_path=./weather_agent/test_config.json --print_detailed_results 
    ```

-   **Evaluate the English Agent**:

    Similarly, you can also evaluate the English version.

    ```bash
    adk eval weather_agent_en/ weather_agent_en/Weather_evaluation.evalset.json
    ```

### Running Tests with Pytest

In addition to running evaluations from the command line, you can also integrate them into your testing workflow using `pytest`. This project includes a test file that demonstrates how to run evaluations programmatically.

**Prerequisites**:

First, install `pytest` and `pytest-asyncio`:
```bash
pip install pytest pytest-asyncio
```

**Running the Tests**:

Execute the following command in the project root directory:
```bash
pytest
```

Pytest will automatically discover and run the tests defined in `tests/test_agent_evaluation.py`, which evaluate both the English and Traditional Chinese agents against their respective evaluation sets. This is useful for integrating agent evaluation into CI/CD pipelines.

### Viewing Evaluation Results

After the evaluation is complete, detailed JSON reports are automatically saved in the `.adk/eval_history/` directory of each agent. You can review these files to analyze the agent's tool usage, responses, and whether they meet expectations.

### Interactive Testing

In addition to automated evaluation, you can also launch a local web interface to interact with your agent in real-time.

-   **Start the interactive UI for the Traditional Chinese Agent**:
    ```bash
    adk web weather_agent/
    ```
-   **Start the interactive UI for the English Agent**:
    ```bash
    adk web weather_agent_en/
    ```

After running the command, the terminal will provide a URL (usually `http://127.0.0.1:8080`). Open it in your browser to start chatting with your agent.

---

## License

This project is licensed under the [Apache 2.0 License](LICENSE).
